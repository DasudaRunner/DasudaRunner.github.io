---
layout: post
title: "CNN网络结构总结[二]"
date: 2019-04-07
categories:
- CNN
tag:
- CNN
- 网络总结

excerpt: 这篇文章主要是总结一些经典的轻量级网络,随着神经网络在终端上的应用,网络的精简化和高效化变得越来越重要.

---
* 目录
{:toc}
这篇文章主要是总结一些经典的轻量级网络,随着神经网络在终端上的应用,网络的精简化和高效化变得越来越重要.轻量级网络大部分是在CNN的卷积方式上做出改变,它们与网络压缩还是有很大的不同,后者是在网络训练完成后做的工作,前者工作是在网络的设计阶段.


## SqueezeNet - 2017

<img src="/assets/images/posts/CNN2/squeezenet.jpg"/>

特点:
- 提出fire module(squeeze层+expand层). squeeze层就是1x1的卷积,但是通道数少于上一层,起到了特征压缩的作用,毕竟轻量级网络,要保证计算量尽可能小. expand层有两条路,一路还是1x1的卷积,一路为3x3的卷积,颇有inception结构的意思,最后将两路的输出拼接在一起,在经过激活函数.
- 不过论文中说模型大小<0.5M(论文名称:SqueezeNet ：AlexNet-level accuracy with 50x fewer parameters and <0.5MB),是进行模型压缩后的数据(标!题!党!)

## Xception

<img src="/assets/images/posts/CNN2/xcepyion.jpg"/>

特点:
- 也是采用深度分离卷积形式,但是和MobileNet V1不同的是,它受inception结构的启发,在depth-wise时采用1x1的卷积核,在point-wise时采用3x3的卷积核.

## MobileNet V1 - 2017

<img src="/assets/images/posts/CNN2/mobilenet1.jpg"/>

特点:
- 采用深度分离卷积代替普通卷积(注意深度分离卷积并不是他提出的).深度分离卷积分为两个步骤:depth-wise和point-wise.这里可以类比group convolution,组卷积最后是采用拼接来完成通道融合,而深度分离卷积是采用一组1x1卷积来进行通道间的融合.另外它还可以看作是组卷积的一种特殊形式(分组数为输入通道数,即每一组只有一个feature map).

<img src="/assets/images/posts/CNN2/mobilenet12.png"/>

- 深度分离卷积使网络维持准确率的前提下,大大减少了网络参数量和计算量(注意参数量少,计算量不一定小).
- 网络中没有使用池化层,而是使用strides为2的卷积代替.

## ShuffleNet V1 - 2017

[图片来源](https://blog.csdn.net/u011974639/article/details/79200559)
<img src="/assets/images/posts/CNN2/shuffle.png"/>

特点:
- 提出channel shuffle,为的是解决group convolution中分组之间信息不互通的问题,在组卷积后进行组重排,强行让信息在分组间流动.

<img src="/assets/images/posts/CNN2/shuffle1.png"/>

- 在ResNet的残差模块上进行改进,将原来的图(a)1x1+bn+relu -> 3x3+bn+relu -> 1x1+ bn  -> add -> relu,结构改为图(b) 1x1 Gconv+ bn+relu - channel shuffle - 3x3+bn -1x1 Gconv+bn - add - relu.图(c)是当输出尺寸小于输入尺寸时,在另外一条支路上添加一个stride为2的池化.

## MobileNet V2

<img src="/assets/images/posts/CNN2/mobilenet2.png"/>

特点:
- 结合了残差模块,但是和残差模块不同的是,它先使用1x1卷积对输入的通道进行扩张,最后再压缩.另外在最后用于压缩的1x1卷积操作后没有激活函数(激活函数在高维特征空间可以有效的增加非线性,但是在较低维度空间时,反而会破坏特征).
- 在输入输出大小不变时,同样采用了shortcut connection,这点和残差模块一致.

## ShuffleNet V2

<img src="/assets/images/posts/CNN2/shuffle2.jpg"/>

特点:
- 提出了FLOPs不能作为衡量目标检测模型运行速度的唯一标准，因为MAC(Memory access cost)也是影响模型运行速度的一大因素.进而作者提出了设计轻量级网络要遵循的准则:
	- 1. 卷积操作时，输入输出采用相同通道数可以降低MAC。
	假设一个1*1卷积层的输入特征通道数是c1，输出特征尺寸是h和w，输出特征通道数是c2，
	$MAC=hw(c_1+c_2)+c_{1}c_{2}$
	$FLOPS=hwc_{1}c_{2}$
	由均值不等式可得:$MAC\ge2\sqrtP{hw*FLOPS}+FLOPS/hw$
	当\sqrt{c_1^2-c_2^2}=0时,即$c_1=c_2$时,MAC取最小值.
	- 2. 在group convolution中,分组过多,会使MAC增加.
	- 3. 分支数量越少,模型速度越快.
	- 4. 尽可能减少element-wise操作.
- 根据以上提出的四条准则,对ShuffleNet V1进行了改进,具体结构如上图(c)(d)所示.
	- 1. 在开始添加一个channel splite操作,将输入通道平均分为两份.对应准则1.
	- 2. 取消了组卷积,对应准则2.
	- 3. 将channel shuffle移到最后,对应准则3.
	- 4. 用拼接操作代替add操作,对应准则4.

## 总结

总结了上述几种网络,大家对深度分离卷积进行各种魔改,大家冥冥之中,都是在解决通道间信息流通不够的问题,,MobileNet及Xception采用卷积的形式(points wise),而ShuffleNet采用channel shuffle的形式,然后都再加上shortcut connection,计算量和准确率都得到了保证.反正感觉这块没有实质性的进展,都是在前几年的研究上修修补补.

## 感想

- 顺着如何加强通道间信息交流的思路,一方面是能不能在chanel shuffle的时候加上个注意力机制,让网络决定哪些信息是需要在通道间流通的,哪些不需要,因为图像毕竟是结构化数据,如果强行让全部通道进行shuffle,会不会破坏某些特征的独特性.
- 从其它方面来加强网络对特征的利用率,感觉深度分离卷积这条路没什么可以做的了.