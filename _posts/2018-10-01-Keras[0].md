---
layout: post
title: "Keras[0]"
date: 2018-10-01
categories:
- keras
tag:
- keras
---

<br>
- 本文为Haibo原创文章，转载请注明：[Haibo的主页](https://dasuda.top)

- 如果对本站的文章有疑问或者有合作需求的，可以联系QQ: 827091497，或者发送邮件到：[haibo.david@qq.com](mailto:haibo.david@qq.com) 。

### Keras中的网络层一览

#### **核心层，这里我们只介绍计算机视觉中图像识别可能使用到的层**

- **Dense**：全连接层
- Dropout：经典的dropout，但现在越来越少用了，理由可以自行查找，破坏分布，特别是网络前面有BN时
- Activation：激活层，在一般的层中会有activation参数来设置激活函数类型
- **Flatten**：展平层，将多维的数据展成一维，例如（3,3,12）-> （108,）
- Reshape：类比numpy中的reshape，符合质量守恒定律，前后的数据量是不变的，**当该层作为首层时**，需要制定input_shape参数
```python
model.add(Reshape((3, 4), input_shape=(12,)))
```
- Permute：维度重排层
- RepeatVector：将输入向量重复并形成新的一批数据，（None,32）- > （None,n,32）
- **Lambda**：自定义函数层，可以对输入的数据进行任意表达式的运算

```python
#第一种：就是对输入数据平方
model.add(Lambda(lambda x: x ** 2))
#第二种：定义更复杂的函数
def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)
    
model.add(Lambda(antirectifier))#当使用tensorflow时，output_shape可自动推断
```

- ActivityRegularizer：目前不清楚有什么作用，文档介绍为：*经过本层的数据不会有任何变化，但会基于其激活值更新损失函数值*
- Masking：是RNN中的用法，暂不做介绍，文档中的介绍为：*使用给定的值对输入的序列信号进行“屏蔽”，用以定位需要跳过的时间步*

#### **卷积层**

- Conv1D：一维滤波，有点像时域上的信号卷积
- **Conv2D**：我们熟悉的CNN中常用的卷积层，二维卷积操作，注意：激活函数不设置默认为恒等变换
- **SeparableConv2D**：高效的深度可分离卷积，将传统卷积分为两步走，巧妙的减少了参数，和Conv2D参数设置一致
- Conv2DTranspose：转置的卷积操作（反卷积）
- Conv3D：三维卷积操作，暂时没有用到
- Cropping1D：对一维的数据进行裁剪
- Cropping2D：对二维图像在长和宽上进行裁剪
- Cropping3D：暂不清楚
- UpSampling1D：上采样层
- UpSampling2D：类比同上
- UpSampling3D：类比同上
- ZeroPadding1D：在一维上进行0填充
- ZeroPadding2D：类比同上
- ZeroPadding3D：类比同上
- MaxPooling1D：一维信号上的最大池化
- **MaxPooling2D**：池化在CNN发展初期很重要的一种层，但是在后期被卷积层逐步替代（strides为2的卷积操作也能降低特征图尺寸，增大特征的感受野，也能学习出类似最大池化的计算形式，但是池化层的计算效率是比卷积高效很多的，就目前来说。）
- MaxPooling3D：对三维信号进行池化操作
- AveragePooling1D：平均池化，同上
- **AveragePooling2D**：平均池化，同上
- AveragePooling3D：平均池化，同上
- GlobalMaxPooling1D：全局最大池化
- **GlobalMaxPooling2D**：这个是现在流行起来的使用GAP或者GMP来代替全连接层，将每一单幅的特征图直接输出为一个值，要么是整张特征图的最大值要么是平均值，听说一片论文还做了实验，通过热图之类的图像，说明了全局池化可以帮助定位到识别的物体。
- GlobalAveragePooling1D：全局平均池化，同上
- **GlobalAveragePooling2D**：全局平均池化，同上

#### **融合层（首字母小写后的函数即为他们对应封装，Add对应的就是add）**

- **Add**：顾名思义就是将多个tensor进行相加，输出shape不变（毋庸置疑），注意：接收的参数是一个列表[x1,x2,...]
- SubStract：输出的是两个tensor的差，输出shape不变，注意：接收的参数是一个列表[x1,x2,...]
- Multiply：接收一个列表，返回他们对应位置的乘积
- Average：接收一个列表，返回对应位置上的均值
- Maximum：接收一个列表，返回对应位置上的最大值
- **Concatenate**：接收一个列表，按照指定的维度进行拼接
- Dot：两个矩阵的张量乘积，注意里面的正则化参数。

#### **高级激活层**

- LeakyReLU：修正线性整流函数，是relu的修正版，relu在网络层的输出值在负值时，是出于死亡状态的，这对网络的性能是不利的，LeakyReLU正好抓住了这一点，在输出小于0时，仍会有输出值，公式为：`f(x)=alpha*x when x<0`，其中`alpha`为认为设置的参数
- **PReLU**：参数化的relu函数，公式和LeakyReLU差不多，但是它的`alpha`是可以学习的
- ELU：指数线性单元，`f(x) = alpha * (exp(x) - 1.) when x < 0`
- ThresholdedReLU：带有门限的激活函数，没用过，`f(x) = x when x > theta, f(x) = 0 otherwise`

#### **批规范化层**
- **BatchNormalization**：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1，注意：其中的center参数要设置为True。一般来说，偏置是单独加在BN层中的。

### Keras中的网络配置一览

#### **初始化方法**
一般来说，权重的初始化为`kernel_initializer`，偏置的初始化为`bias_initializer`，后面既可以跟具体的函数名，如：`kernel_initializer=initializers.random_normal(stddev=0.01)`，可以跟预定义的字符串，如：`kernel_initializer='random_normal')`，但是区别已经很显而易见了，使用具体的函数时可以设置参数。

- Zeros：全0初始化
- Ones：全1初始化
- Constant：初始化为固定值
- RandomNormal：正态分布初始化，当然需要设置均值和方差
- RandomUniform：均匀分布
- **TruncatedNormal**：截断的正态分布，位于两个标准差之外会被舍弃，**该方法在之前是网络的标准初始化方式**
- VarianceScaling：当scale为2.0时，即为he_normal
- Orthogonal：随机正交矩阵进行初始化，暂时没用到过
- Identiy：单位矩阵初始化，暂时没用到过
- lecun_uniform：[-limit, limit]的区间中均匀采样，其中limit=sqrt(3/fan\_in)
- lecun_normal：由0均值，标准差为stddev = sqrt(1 / fan\_in)的正态分布产生
- glorot_normal：就是Xavier正态分布初始化，与sigmoid函数配合使用
- **he_normal**：名的Kaiming初始化，和relu搭配之用效果更佳，它是一种自适应的初始化方法，根据输入和输出神经元的个数来决定截断正态分布的方差，可配置为三种模式：`fan_in`，`fan_out`,`fan_avg`，[推导](https://blog.csdn.net/VictoriaW/article/details/73166752)

#### **激活函数**
Keras中可以使用单独的激活层，也可以在网络层中传递`activation`参数来实现

- **softmax**
- **relu**
- tanh
- sigmoid
- hard_sigmoid
- linear
- elu
- selu：可伸缩的指数线性单元
- softplus
- softsign

#### **正则项**
- **kernel_regularizer**：施加在权重上的惩罚项
- **bias_regularizer**：施加在偏置上的惩罚项
- activity_regularizer：施加在输出上的惩罚项，这个没用过

#### **损失函数**
- mean_squared_error或mse
- mean_absolute_error或mae
- mean_absolute_percentage_error或mape
- mean_squared_logarithmic_error或msle
- squared_hinge
- hinge
- categorical_hinge
- binary_crossentropy
- logcosh
- categorical\_crossentropy：softmax交叉熵损失函数，标签为one_hot类型，可用

```python
from keras.utils.np_utils import to_categorical

categorical_labels = to_categorical(int_labels, num_classes=None)
```

- sparse\_categorical\_crossentrop
- kullback\_leibler\_divergence：预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异
- poisson
- cosine_proximity：预测值与真实标签的余弦距离平均值的相反数

#### **优化算法**

- **SGD**：随机梯度下降法
- RMSprop：常用在递归神经网络中，建议除学习率外，其他参数保持默认
- Adagrad：保持默认参数
- Adadelta：保持默认参数
- **Adam**：比较常用的优化器，保持默认参数
- Adamax：Adamax优化器来自于Adam的论文的Section7，该方法是基于无穷范数的Adam方法的变体
- Nadam：Adam本质上像是带有动量项的RMSprop，Nadam就是带有Nesterov 动量的Adam RMSprop

#### **性能评估**
在compile我们常常还会设置一个参数`metrics`，它是用来制定评估方式。性能评估函数类似与目标函数, 只不过该性能的评估结果讲不会用于训练.

- **binary_accuracy**：对二分类问题,计算在所有预测值上的平均正确率
- **categorical_accuracy**： 对多分类问题,计算再所有预测值上的平均正确率
- **top_k_categorical_accracy**：计算top-k正确率,当预测值的前k个值中存在目标类别即认为预测正确，在一些比赛中会让你给出top-k的准确率


### Keras中的预训练模型

模型的预训练权重将下载到~/.keras/models/，并在载入模型时自动载入

| **模型**|**大小**|**Top-1准确率**|**Top-5准确率**|**参数数目**|**深度**|
| :--:  |:--: | :--: | :--: | :--: | :--: |
|Xception|88MB|0.790|0.945|22,910,480|126|
|VGG16|528MB|0.715|0.901|138,357,544|23|
|VGG19|549MB|0.727|0.910|143,667,240|26|
|ResNet50|99MB|0.759|0.929|25,636,712|168|
|InceptionV3|92MB|0.788|0.944|23,851,784|159|
|MobileNet|17MB|0.665|0.871|4,253,864|88|
